{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing:\n",
    "* Global Signal Regression using orthogonalization\n",
    "* Band Pass filtering 0.1 - 0.01 Hz\n",
    "* Motion regression using GLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit.version'\n"
     ]
    }
   ],
   "source": [
    "from bids.grabbids import BIDSLayout\n",
    "from nipype.interfaces.fsl import (BET, ExtractROI, FAST, FLIRT, ImageMaths,\n",
    "                                   MCFLIRT, SliceTimer, Threshold,Info, ConvertXFM,MotionOutliers)\n",
    "from nipype.interfaces.afni import Resample\n",
    "from nipype.interfaces.io import DataSink\n",
    "from nipype.pipeline import Node, MapNode, Workflow, JoinNode\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "import os\n",
    "from os.path import join as opj\n",
    "from nipype.interfaces import afni\n",
    "import nibabel as nib\n",
    "import json    \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "path_cwd = os.getcwd()\n",
    "path_split_list = path_cwd.split('/')\n",
    "s = path_split_list[0:-1] # for getting to the parent dir of pwd\n",
    "s = opj('/',*s) # *s converts list to path, # very important to add '/' in the begining so it is read as directory later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# json_path = opj(data_directory,'task-rest_bold.json')\n",
    "\n",
    "json_path = 'scripts/json/paths.json'\n",
    "with open(json_path, 'rt') as fp:\n",
    "    task_info = json.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_directory = opj(s,'result') \n",
    "# parent_wf_directory = 'preprocessPipeline_ABIDE2_GU1_withfloat'\n",
    "# child_wf_directory = 'coregistrationPipeline'\n",
    "\n",
    "# data_directory = opj(s,\"data/ABIDE2-BIDS/GU1\")\n",
    "\n",
    "# datasink_name = 'datasink_preprocessed_ABIDE2_GU1_withfloat'\n",
    "\n",
    "base_directory = opj(s,task_info[\"base_directory_for_results\"]) \n",
    "motion_correction_bet_directory = task_info[\"motion_correction_bet_directory\"]\n",
    "parent_wf_directory = task_info[\"parent_wf_directory\"]\n",
    "# functional_connectivity_directory = task_info[\"functional_connectivity_directory\"]\n",
    "functional_connectivity_directory = 'temp_fc'\n",
    "coreg_reg_directory = task_info[\"coreg_reg_directory\"]\n",
    "atlas_resize_reg_directory = task_info[\"atlas_resize_reg_directory\"]\n",
    "data_directory = opj(s,task_info[\"data_directory\"])\n",
    "datasink_name = task_info[\"datasink_name\"]\n",
    "fc_datasink_name = task_info[\"fc_datasink_name\"]\n",
    "fc_datasink_name = 'temp_dataSink'\n",
    "atlasPath = opj(s,task_info[\"atlas_path\"])\n",
    "\n",
    "\n",
    "# mask_file = '/media/varun/LENOVO4/Projects/result/preprocessPipeline/coregistrationPipeline/_subject_id_0050952/skullStrip/sub-0050952_T1w_resample_brain_mask.nii.gz'\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opj(base_directory,parent_wf_directory,motion_correction_bet_directory,coreg_reg_directory,'resample_mni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain_path = opj(base_directory,datasink_name,'preprocessed_brain_paths/brain_file_list.npy')\n",
    "mask_path = opj(base_directory,datasink_name,'preprocessed_mask_paths/mask_file_list.npy')\n",
    "atlas_path = opj(base_directory,datasink_name,'atlas_paths/atlas_file_list.npy')\n",
    "tr_path = opj(base_directory,datasink_name,'tr_paths/tr_list.npy')\n",
    "motion_params_path = opj(base_directory,datasink_name,'motion_params_paths/motion_params_file_list.npy')\n",
    "\n",
    "func2std_mat_path = opj(base_directory, datasink_name,'joint_xformation_matrix_paths/joint_xformation_matrix_file_list.npy')\n",
    "\n",
    "MNI3mm_path = opj(base_directory,parent_wf_directory,motion_correction_bet_directory,coreg_reg_directory,'resample_mni/MNI152_T1_2mm_brain_resample.nii') \n",
    "\n",
    "# brain_list = np.load('../results_again_again/ABIDE1_Preprocess_Datasink/preprocessed_brain_paths/brain_file_list.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brain_path,mask_path,atlas_path,tr_path,motion_params_path,func2std_mat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain_path = np.load(brain_path)\n",
    "mask_path = np.load(mask_path)\n",
    "atlas_path = np.load(atlas_path)\n",
    "tr_path = np.load(tr_path)\n",
    "motion_params_path = np.load(motion_params_path)\n",
    "func2std_mat_path = np.load(func2std_mat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a,b,c,d,e in zip(brain_path,mask_path,atlas_path,tr_path,motion_params_path):\n",
    "#     print (a,b,c,d,e,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layout = BIDSLayout(data_directory)\n",
    "\n",
    "number_of_subjects = 2 # Number of subjects you wish to preprocess\n",
    "# number_of_subjects = len(layout.get_subjects())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Data directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(layout.get_subjects()) # working!Gives us list of all the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layout.get_subjects();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the metadata associated with a subject. [Takes as argumment the filename of subject ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_list = (layout.get_subjects())[0:number_of_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layout.get();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our own custom function - BIDSDataGrabber using a Function Interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_nifti_filenames(subject_id,data_dir):\n",
    "# #     Remember that all the necesary imports need to be INSIDE the function for the Function Interface to work!\n",
    "#     from bids.grabbids import BIDSLayout\n",
    "    \n",
    "#     layout = BIDSLayout(data_dir)\n",
    "    \n",
    "#     anat_file_path = [f.filename for f in layout.get(subject=subject_id, type='T1w', extensions=['nii', 'nii.gz'])]\n",
    "#     func_file_path = [f.filename for f in layout.get(subject=subject_id, type='bold', run='1', extensions=['nii', 'nii.gz'])]\n",
    "    \n",
    "#     return anat_file_path[0],func_file_path[0]\n",
    "\n",
    "# # Refer to Supplementary material section One for info on arguments for layout.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap it inside a Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BIDSDataGrabber = Node(Function(function=get_nifti_filenames, input_names=['subject_id','data_dir'],\n",
    "#                                 output_names=['anat_file_path','func_file_path']), name='BIDSDataGrabber')\n",
    "# # BIDSDataGrabber.iterables = [('subject_id',subject_list)]\n",
    "# BIDSDataGrabber.inputs.data_dir = data_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(subject_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to fetch the filenames of a particular subject ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject_filenames(subject_id,brain_path,mask_path,atlas_path,tr_path,motion_params_path,func2std_mat_path,MNI3mm_path):\n",
    "    import re\n",
    "\n",
    "    for brain,mask,atlas,tr,motion_param,func2std_mat in zip(brain_path,mask_path,atlas_path,tr_path,motion_params_path,func2std_mat_path):\n",
    "        sub_id_extracted = re.search('.+_subject_id_(\\d+)', brain).group(1)\n",
    "        if subject_id == sub_id_extracted:\n",
    "#             print(\"Files for subject \",subject_id,brain,mask,atlas,tr,motion_param)\n",
    "            return brain,mask,atlas,tr,motion_param,func2std_mat,MNI3mm_path\n",
    "        \n",
    "    print ('Unable to locate Subject: ',subject_id,'extracted: ',sub_id_extracted)\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a node\n",
    "getSubjectFilenames = Node(Function(function=get_subject_filenames, input_names=['subject_id','brain_path','mask_path','atlas_path','tr_path','motion_params_path','func2std_mat_path','MNI3mm_path'],\n",
    "                                output_names=['brain','mask','atlas','tr','motion_param','func2std_mat', 'MNI3mm_path']), name='getSubjectFilenames')\n",
    "\n",
    "\n",
    "getSubjectFilenames.inputs.brain_path = brain_path\n",
    "getSubjectFilenames.inputs.mask_path = mask_path\n",
    "getSubjectFilenames.inputs.atlas_path = atlas_path\n",
    "getSubjectFilenames.inputs.tr_path = tr_path\n",
    "getSubjectFilenames.inputs.motion_params_path = motion_params_path\n",
    "getSubjectFilenames.inputs.func2std_mat_path = func2std_mat_path\n",
    "getSubjectFilenames.inputs.MNI3mm_path = MNI3mm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# text = '/home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/coreg_reg/atlas_resize_reg_directory/_subject_id_0050004/111std2func_xform/fullbrain_atlas_thr0-2mm_resample_flirt.nii'\n",
    "\n",
    "# try:\n",
    "#     found = re.search('.+_subject_id_(\\d+)', text).group(1)\n",
    "# except AttributeError:\n",
    "#     # AAA, ZZZ not found in the original string\n",
    "#     found = '' # apply your error handling\n",
    "\n",
    "# # found: 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infosource = Node(IdentityInterface(fields=['subject_id']),\n",
    "                  name=\"infosource\")\n",
    "\n",
    "infosource.iterables = [('subject_id',subject_list)]\n",
    "\n",
    "\n",
    "# ,'brain_path','mask_path','atlas_path','tr_path','motion_params_path'\n",
    "# infosource.brain_path = brain_path\n",
    "# infosource.mask_path = mask_path\n",
    "# infosource.atlas_path = atlas_path\n",
    "# infosource.tr_path = tr_path\n",
    "# infosource.motion_params_path = motion_params_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Band Pass Filtering\n",
    "Let's do a band pass filtering on the data using the code from https://neurostars.org/t/bandpass-filtering-different-outputs-from-fsl-and-nipype-custom-function/824/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AFNI\n",
    "\n",
    "bandpass = Node(afni.Bandpass(highpass=0.01, lowpass=0.1, \n",
    "                         despike=False, no_detrend=True, notrans=True, \n",
    "                         outputtype='NIFTI_GZ'),name='bandpass')\n",
    "\n",
    "# bandpass = Node(afni.Bandpass(highpass=0.001, lowpass=0.01, \n",
    "#                          despike=False, no_detrend=True, notrans=True, \n",
    "#                          tr=2.0,outputtype='NIFTI_GZ'),name='bandpass')\n",
    "\n",
    "\n",
    "# bandpass.inputs.mask = MNI152_2mm.outputs.mask_file\n",
    "\n",
    "# highpass=0.008, lowpass=0.08, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing bandpass on the func data in subject's space\n",
    "\n",
    "# First comment out the bandpass.inputs.mask as it is in standard space.\n",
    "\n",
    "# subject_id = layout.get_subjects()[0] # gives the first subject's ID\n",
    "# func_file_path = [f.filename for f in layout.get(subject=subject_id, type='bold', extensions=['nii', 'nii.gz'])] \n",
    "# bandpass.inputs.in_file = func_file_path[0]\n",
    "# res = bandpass.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# res.outputs.out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Highpass filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://afni.nimh.nih.gov/pub/dist/doc/program_help/3dBandpass.html\n",
    "# os.chdir('/home1/varunk/Autism-Connectome-Analysis-bids-related/')\n",
    "highpass = Node(afni.Bandpass(highpass=0.01, lowpass=99999, \n",
    "                         despike=False, no_detrend=True, notrans=True, \n",
    "                         outputtype='NIFTI_GZ'),name='highpass')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test\n",
    "\n",
    "# subject_id = layout.get_subjects()[0] # gives the first subject's ID\n",
    "# func_file_path = [f.filename for f in layout.get(subject=subject_id, type='bold', extensions=['nii', 'nii.gz'])] \n",
    "# highpass.inputs.in_file = func_file_path[0]\n",
    "# res = highpass.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing \n",
    "### Using 6mm fwhm\n",
    "sigma = 6/2.3548 = 2.547987090198743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db2476687e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spatial_smooth = Node(interface=ImageMaths(op_string='-s 2.5479',\n\u001b[0m\u001b[1;32m      2\u001b[0m                                             suffix='_smoothed'),\n\u001b[1;32m      3\u001b[0m                    name='spatial_smooth')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Node' is not defined"
     ]
    }
   ],
   "source": [
    "spatialSmooth = Node(interface=ImageMaths(op_string='-s 2.5479',\n",
    "                                            suffix='_smoothed'),\n",
    "                   name='spatialSmooth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performs Gram Schmidt Process\n",
    "https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orthogonalize(in_file, mask_file):\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    import os\n",
    "    from os.path import join as opj\n",
    "    \n",
    "    def gram_schmidt(voxel_time_series, mean_vector):\n",
    "        numerator = np.dot(voxel_time_series,mean_vector)\n",
    "        dinominator = np.dot(mean_vector,mean_vector)\n",
    "        voxel_time_series_orthogonalized = voxel_time_series - (numerator/dinominator)*mean_vector\n",
    "        \n",
    "#         TO CONFIRM IF THE VECTORS ARE ORTHOGONAL\n",
    "#         sum_dot_prod = np.sum(np.dot(voxel_time_series_orthogonalized,mean_vector))\n",
    "        \n",
    "#         print('Sum of entries of orthogonalized vector = ',sum_dot_prod)\n",
    "        return voxel_time_series_orthogonalized\n",
    "    \n",
    "    \n",
    "    mask_data = nib.load(mask_file)\n",
    "    mask = mask_data.get_data()\n",
    "    \n",
    "    brain_data = nib.load(in_file)\n",
    "    brain = brain_data.get_data()\n",
    "\n",
    "    x_dim, y_dim, z_dim, t_dim = brain_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find mean brain\n",
    "    \n",
    "    \n",
    "    mean_vector = np.zeros(t_dim)\n",
    "    \n",
    "    \n",
    "    num_brain_voxels = 0\n",
    "    \n",
    "    # Count the number of brain voxels\n",
    "    for i in range(x_dim):\n",
    "        for j in range(y_dim):\n",
    "            for k in range(z_dim):\n",
    "                if mask[i,j,k] == 1:\n",
    "                    mean_vector = mean_vector + brain[i,j,k,:]\n",
    "                    num_brain_voxels = num_brain_voxels + 1\n",
    "                    \n",
    "     \n",
    "    mean_vector = mean_vector / num_brain_voxels\n",
    "    \n",
    "    # Orthogonalize\n",
    "    for i in range(x_dim):\n",
    "        for j in range(y_dim):\n",
    "            for k in range(z_dim):\n",
    "                if mask[i,j,k] == 1:\n",
    "                    brain[i,j,k,:] = gram_schmidt(brain[i,j,k,:], mean_vector)\n",
    "                    \n",
    "    \n",
    "    \n",
    "    sub_id = in_file.split('/')[-1].split('.')[0].split('_')[0].split('-')[1]\n",
    "    \n",
    "    gsr_file_name = 'sub-' + sub_id + '_task-rest_run-1_bold.nii.gz'\n",
    "    \n",
    "#     gsr_file_name_nii = gsr_file_name + '.nii.gz'\n",
    "    \n",
    "    out_file = opj(os.getcwd(),gsr_file_name) # path\n",
    "    \n",
    "    brain_with_header = nib.Nifti1Image(brain, affine=brain_data.affine,header = brain_data.header)\n",
    "    nib.save(brain_with_header,gsr_file_name)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "                    \n",
    "                    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globalSignalRemoval = Node(Function(function=orthogonalize, input_names=['in_file','mask_file'], \n",
    "                                  output_names=['out_file']), name='globalSignalRemoval' )\n",
    "# globalSignalRemoval.inputs.mask_file = mask_file\n",
    "# globalSignalRemoval.iterables = [('in_file',file_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM for regression of motion parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_residuals(subject,\n",
    "                   motion_file):\n",
    "    \"\"\"\n",
    "    Calculates residuals of nuisance regressors -motion parameters for every voxel for a subject using GLM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject : string\n",
    "        Path of a subject's motion corrected nifti file.\n",
    "    motion_par_file : string\n",
    "        path of a subject's motion parameters\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    residual_file : string\n",
    "        Path of residual file in nifti format\n",
    "    \n",
    "    \"\"\"\n",
    "    import nibabel as nb\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from os.path import join as opj\n",
    "    nii = nb.load(subject)\n",
    "    data = nii.get_data().astype(np.float32)\n",
    "    global_mask = (data != 0).sum(-1) != 0\n",
    "\n",
    "    \n",
    "    # Check and define regressors which are provided from files\n",
    "    if motion_file is not None:\n",
    "        motion = np.genfromtxt(motion_file)\n",
    "        if motion.shape[0] != data.shape[3]:\n",
    "            raise ValueError('Motion parameters {0} do not match data '\n",
    "                             'timepoints {1}'.format(motion.shape[0], \n",
    "                                                     data.shape[3]))\n",
    "        if motion.size == 0:\n",
    "            raise ValueError('Motion signal file {0} is '\n",
    "                             'empty'.format(motion_file))\n",
    "\n",
    "    # Calculate regressors\n",
    "    regressor_map = {'constant' : np.ones((data.shape[3],1))}\n",
    "        \n",
    "    regressor_map['motion'] = motion\n",
    "        \n",
    "    \n",
    "    X = np.zeros((data.shape[3], 1))\n",
    "    \n",
    "    for rname, rval in regressor_map.items():\n",
    "        X = np.hstack((X, rval.reshape(rval.shape[0],-1)))\n",
    "\n",
    "    X = X[:,1:]\n",
    "    \n",
    "    if np.isnan(X).any() or np.isnan(X).any():\n",
    "        raise ValueError('Regressor file contains NaN')\n",
    "\n",
    "    Y = data[global_mask].T\n",
    "\n",
    "    try:\n",
    "        B = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        if \"Singular matrix\" in e:\n",
    "            raise Exception(\"Error details: {0}\\n\\nSingular matrix error: \"\n",
    "                            \"The nuisance regression configuration you \"\n",
    "                            \"selected may have been too stringent, and the \"\n",
    "                            \"regression could not be completed. Ensure your \"\n",
    "                            \"parameters are not too \"\n",
    "                            \"extreme.\\n\\n\".format(e))\n",
    "        else:\n",
    "            raise Exception(\"Error details: {0}\\n\\nSomething went wrong with \"\n",
    "                            \"nuisance regression.\\n\\n\".format(e))\n",
    "\n",
    "    Y_res = Y - X.dot(B)\n",
    "    \n",
    "    data[global_mask] = Y_res.T\n",
    "    \n",
    "    img = nb.Nifti1Image(data, header=nii.get_header(),\n",
    "                         affine=nii.get_affine())\n",
    "    \n",
    "    subject_name = subject.split('/')[-1].split('.')[0]\n",
    "    filename = subject_name + '_residual.nii.gz'\n",
    "    residual_file = os.path.join(os.getcwd(),filename )\n",
    "    img.to_filename(residual_file) # alt to nib.save\n",
    "    \n",
    "    return residual_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Node for above\n",
    "calc_residuals = Node(Function(function=calc_residuals, input_names=['subject','motion_file'],\n",
    "                                output_names=['residual_file']), name='calc_residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasink\n",
    "I needed to define the structure of what files are saved and where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create DataSink object\n",
    "dataSink = Node(DataSink(), name='datasink')\n",
    "\n",
    "# Name of the output folder\n",
    "dataSink.inputs.base_directory = opj(base_directory,fc_datasink_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the substitutions I looked the `datasink` folder where I was redirecting the output. I manually selected the part of file/folder name that I wanted to change and copied below to be substituted.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define substitution strings so that the data is similar to BIDS\n",
    "substitutions = [('_subject_id_', 'sub-')]\n",
    "\n",
    "# Feed the substitution strings to the DataSink node\n",
    "dataSink.inputs.substitutions = substitutions\n",
    "\n",
    "# ('_resample_brain_flirt.nii_brain', ''),\n",
    "# ('_roi_st_mcf_flirt.nii_brain_flirt', ''),\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home1/varunk/results_again_again'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is a Join Node that collects the preprocessed file paths and saves them in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file_list_function(in_fc_map_brain_file):\n",
    "    # Imports\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from os.path import join as opj\n",
    "    \n",
    "    \n",
    "    file_list = np.asarray(in_fc_map_brain_file)\n",
    "    print('######################## File List ######################: \\n',file_list)\n",
    "\n",
    "    np.save('fc_map_brain_file_list',file_list)\n",
    "    file_name = 'fc_map_brain_file_list.npy'\n",
    "    out_fc_map_brain_file = opj(os.getcwd(),file_name) # path\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return out_fc_map_brain_file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file_list = JoinNode(Function(function=save_file_list_function, input_names=['in_fc_map_brain_file'],\n",
    "                 output_names=['out_fc_map_brain_file']),\n",
    "                 joinsource=\"infosource\",\n",
    "                 joinfield=['in_fc_map_brain_file'],\n",
    "                 name=\"save_file_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a FC node\n",
    "\n",
    "This node:\n",
    "1. Exracts the average time series of the brain ROI's using the atlas and stores \n",
    "    it as a matrix of size [ROIs x Volumes].\n",
    "2. Extracts the Voxel time series and stores it in matrix of size [Voxels x Volumes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def pear_coff(in_file, atlas_file, mask_file):\n",
    "#     # code to find how many voxels are in the brain region using the mask\n",
    "    \n",
    "#         # imports\n",
    "#     import numpy as np\n",
    "#     import nibabel as nib\n",
    "#     import os\n",
    "#     from os.path import join as opj\n",
    "\n",
    "#     mask_data = nib.load(mask_file)\n",
    "#     mask = mask_data.get_data()\n",
    "\n",
    "#     x_dim, y_dim, z_dim = mask_data.shape\n",
    "\n",
    "                    \n",
    "#     atlasPath = atlas_file\n",
    "#     # Read the atlas\n",
    "#     atlasObject = nib.load(atlasPath)\n",
    "#     atlas = atlasObject.get_data()\n",
    "    \n",
    "#     num_ROIs = int((np.max(atlas) - np.min(atlas) ))\n",
    "\n",
    "\n",
    "#     # Read the brain in_file\n",
    "\n",
    "#     brain_data = nib.load(in_file)\n",
    "#     brain = brain_data.get_data()\n",
    "\n",
    "#     x_dim, y_dim, z_dim, num_volumes = brain.shape\n",
    "    \n",
    "    \n",
    "#     num_brain_voxels = 0\n",
    "\n",
    "#     x_dim, y_dim, z_dim = mask_data.shape\n",
    "\n",
    "#     for i in range(x_dim):\n",
    "#         for j in range(y_dim):\n",
    "#             for k in range(z_dim):\n",
    "#                 if mask[i,j,k] == 1:\n",
    "#                     num_brain_voxels = num_brain_voxels + 1\n",
    "    \n",
    "#     # Initialize a matrix of ROI time series and voxel time series\n",
    "\n",
    "#     ROI_matrix = np.zeros((num_ROIs, num_volumes))\n",
    "#     voxel_matrix = np.zeros((num_brain_voxels, num_volumes))\n",
    "    \n",
    "#     # Fill up the voxel_matrix \n",
    "\n",
    "#     voxel_counter = 0\n",
    "#     for i in range(x_dim):\n",
    "#         for j in range(y_dim):\n",
    "#             for k in range(z_dim):\n",
    "#                 if mask[i,j,k] == 1:\n",
    "#                     voxel_matrix[voxel_counter,:] = brain[i,j,k,:] \n",
    "#                     voxel_counter = voxel_counter + 1\n",
    "\n",
    "                    \n",
    "#     # Fill up the ROI_matrix\n",
    "#     # Keep track of number of voxels per ROI as well by using an array - num_voxels_in_ROI[]\n",
    "\n",
    "#     num_voxels_in_ROI = np.zeros((num_ROIs,1)) # A column arrray containing number of voxels in each ROI\n",
    "\n",
    "#     for i in range(x_dim):\n",
    "#         for j in range(y_dim):\n",
    "#             for k in range(z_dim):\n",
    "#                 label = int(atlas[i,j,k]) - 1\n",
    "#                 if label != -1:\n",
    "#                     ROI_matrix[label,:] = np.add(ROI_matrix[label,:], brain[i,j,k,:])\n",
    "#                     num_voxels_in_ROI[label,0] = num_voxels_in_ROI[label,0] + 1\n",
    "\n",
    "#     ROI_matrix = np.divide(ROI_matrix,num_voxels_in_ROI) # Check if divide is working correctly\n",
    "\n",
    "#     X, Y = ROI_matrix, voxel_matrix\n",
    "\n",
    "\n",
    "#     # Subtract mean from X and Y\n",
    "\n",
    "#     X = np.subtract(X, np.mean(X, axis=1, keepdims=True))\n",
    "#     Y = np.subtract(Y, np.mean(Y, axis=1, keepdims=True))\n",
    "\n",
    "#     temp1 = np.dot(X,Y.T)\n",
    "#     temp2 = np.sqrt(np.sum(np.multiply(X,X), axis=1, keepdims=True))\n",
    "#     temp3 = np.sqrt(np.sum(np.multiply(Y,Y), axis=1, keepdims=True))\n",
    "#     temp4 = np.dot(temp2,temp3.T)\n",
    "#     coff_matrix = np.divide(temp1, (temp4 + 1e-7))\n",
    "#     print (\"Pear Done\")\n",
    "\n",
    "\n",
    "#     sub_id = in_file.split('/')[-1].split('.')[0].split('_')[0].split('-')[1]\n",
    "    \n",
    "#     fc_file_name = sub_id + '_fc_map'\n",
    "    \n",
    "#     np.save(fc_file_name, coff_matrix)\n",
    "    \n",
    "    \n",
    "#     fc_file_name = fc_file_name + '.npy'\n",
    "    \n",
    "#     fc_map_brain_file = opj(os.getcwd(),fc_file_name) # path\n",
    "#     return fc_map_brain_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Brains instead of FC matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def convert_to_brain(roi_brain_matrix, brain_file, mask_file, fc_file_name): #, brain_voxel_list\n",
    "#     mask_data = nib.load(mask_file)\n",
    "#     mask = mask_data.get_data()\n",
    "    \n",
    "#     # to use the deader of an anatomical file tht has float datatype. mask had uint datatype that made all the p-values zero\n",
    "# #     brain_file = '/home1/varunk/data/ABIDE-BIDS-Preprocessed/NYU/sub-0050952/anat/sub-0050952_T1w.nii.gz'\n",
    "#     brain_data = nib.load(brain_file)\n",
    "    \n",
    "    \n",
    "#     # Empty brain to store the voxel values\n",
    "#     brain = brain_data.get_data()\n",
    "#     brain.fill(0.0)\n",
    "    \n",
    "# #     brain_corrected_mask = np.zeros((brain.shape))\n",
    "\n",
    "\n",
    "# #     roi_number = int(brain_voxel_list[0]) - 1\n",
    "# #     brain_voxel_list = brain_voxel_list[1:]\n",
    "\n",
    "\n",
    "#     x_dim, y_dim, z_dim, t_dim = brain.shape\n",
    "    \n",
    "#     (brain_data.header).set_data_shape([x_dim,y_dim,z_dim,num_ROIs])\n",
    "    \n",
    "#     number_of_rois = roi_brain_matrix.shape[0]\n",
    "    \n",
    "#     brain_roi_tensor = m.np_zeros((brain_data.header.get_data_shape()))\n",
    "    \n",
    "#     for roi in range(number_of_rois):\n",
    "#         brain_voxel_counter = 0\n",
    "#         for i in range(x_dim):\n",
    "#             for j in range(y_dim):\n",
    "#                 for k in range(z_dim):\n",
    "#                     if mask[i,j,k] == 1:\n",
    "#                         brain[i,j,k] = brain_voxel_list[brain_voxel_counter]\n",
    "#                         brain_voxel_counter = brain_voxel_counter + 1\n",
    "\n",
    "#         assert (brain_voxel_counter == len(roi_brain_matrix[roi,:])) \n",
    "#         brain_roi_tensor[:,:,:,roi] = brain\n",
    "#         print(\"Job Done for ROI-\",roi_number)\n",
    "        \n",
    "        \n",
    "#     brain_with_header = nib.Nifti1Image(brain_roi_tensor, affine=brain_data.affine,header = brain_data.header)\n",
    "#     nib.save(brain_with_header,fc_file_name)\n",
    "\n",
    "#     # Saving the brain file\n",
    "\n",
    "#     path = os.getcwd()\n",
    "\n",
    "\n",
    "# #     in_file_split_list = in_file.split('/')\n",
    "# #     in_file_name = in_file_split_list[-1]\n",
    "\n",
    "# #     out_file = in_file_name + '_brain.nii.gz' # changing name\n",
    "# #     brain_with_header = nib.Nifti1Image(brain, affine=brain_data.affine,header = brain_data.header)\n",
    "# #     nib.save(brain_with_header,out_file)\n",
    "\n",
    "#     out_file = opj(path,fc_file_name)\n",
    "\n",
    "#     return out_file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saves the brains instead of FC matrix files\n",
    "def pear_coff(in_file, atlas_file, mask_file):\n",
    "    # code to find how many voxels are in the brain region using the mask\n",
    "    \n",
    "        # imports\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    import os\n",
    "    from os.path import join as opj\n",
    "\n",
    "    mask_data = nib.load(mask_file)\n",
    "    mask = mask_data.get_data()\n",
    "\n",
    "    x_dim, y_dim, z_dim = mask_data.shape\n",
    "\n",
    "                    \n",
    "    atlasPath = atlas_file\n",
    "    # Read the atlas\n",
    "    atlasObject = nib.load(atlasPath)\n",
    "    atlas = atlasObject.get_data()\n",
    "    \n",
    "    num_ROIs = int((np.max(atlas) - np.min(atlas) ))\n",
    "\n",
    "\n",
    "    # Read the brain in_file\n",
    "\n",
    "    brain_data = nib.load(in_file)\n",
    "    brain = brain_data.get_data()\n",
    "\n",
    "    x_dim, y_dim, z_dim, num_volumes = brain.shape\n",
    "    \n",
    "    \n",
    "    num_brain_voxels = 0\n",
    "\n",
    "    x_dim, y_dim, z_dim = mask_data.shape\n",
    "\n",
    "    for i in range(x_dim):\n",
    "        for j in range(y_dim):\n",
    "            for k in range(z_dim):\n",
    "                if mask[i,j,k] == 1:\n",
    "                    num_brain_voxels = num_brain_voxels + 1\n",
    "    \n",
    "    # Initialize a matrix of ROI time series and voxel time series\n",
    "\n",
    "    ROI_matrix = np.zeros((num_ROIs, num_volumes))\n",
    "    voxel_matrix = np.zeros((num_brain_voxels, num_volumes))\n",
    "    \n",
    "    # Fill up the voxel_matrix \n",
    "\n",
    "    voxel_counter = 0\n",
    "    for i in range(x_dim):\n",
    "        for j in range(y_dim):\n",
    "            for k in range(z_dim):\n",
    "                if mask[i,j,k] == 1:\n",
    "                    voxel_matrix[voxel_counter,:] = brain[i,j,k,:] \n",
    "                    voxel_counter = voxel_counter + 1\n",
    "\n",
    "                    \n",
    "    # Fill up the ROI_matrix\n",
    "    # Keep track of number of voxels per ROI as well by using an array - num_voxels_in_ROI[]\n",
    "\n",
    "    num_voxels_in_ROI = np.zeros((num_ROIs,1)) # A column arrray containing number of voxels in each ROI\n",
    "\n",
    "    for i in range(x_dim):\n",
    "        for j in range(y_dim):\n",
    "            for k in range(z_dim):\n",
    "                label = int(atlas[i,j,k]) - 1\n",
    "                if label != -1:\n",
    "                    ROI_matrix[label,:] = np.add(ROI_matrix[label,:], brain[i,j,k,:])\n",
    "                    num_voxels_in_ROI[label,0] = num_voxels_in_ROI[label,0] + 1\n",
    "\n",
    "    ROI_matrix = np.divide(ROI_matrix,num_voxels_in_ROI) # Check if divide is working correctly\n",
    "\n",
    "    X, Y = ROI_matrix, voxel_matrix\n",
    "\n",
    "\n",
    "    # Subtract mean from X and Y\n",
    "\n",
    "    X = np.subtract(X, np.mean(X, axis=1, keepdims=True))\n",
    "    Y = np.subtract(Y, np.mean(Y, axis=1, keepdims=True))\n",
    "\n",
    "    temp1 = np.dot(X,Y.T)\n",
    "    temp2 = np.sqrt(np.sum(np.multiply(X,X), axis=1, keepdims=True))\n",
    "    temp3 = np.sqrt(np.sum(np.multiply(Y,Y), axis=1, keepdims=True))\n",
    "    temp4 = np.dot(temp2,temp3.T)\n",
    "    coff_matrix = np.divide(temp1, (temp4 + 1e-7))\n",
    "    \n",
    "    \n",
    "    # Check if any ROI is missing and replace the NAN values in coff_matrix by 0\n",
    "    if np.argwhere(np.isnan(coff_matrix)).shape[0] != 0:\n",
    "        print(\"Some ROIs are not present. Replacing NAN in coff matrix by 0\")\n",
    "        np.nan_to_num(coff_matrix, copy=False)\n",
    "\n",
    "    # TODO: when I have added 1e-7 in the dinominator, then why did I feel the need to replace NAN by zeros \n",
    "    sub_id = in_file.split('/')[-1].split('.')[0].split('_')[0].split('-')[1]\n",
    "    \n",
    "    \n",
    "    fc_file_name = sub_id + '_fc_map'\n",
    "    \n",
    "    print (\"Pear Matrix calculated for subject: \",sub_id)\n",
    "\n",
    "    roi_brain_matrix = coff_matrix\n",
    "    brain_file = in_file\n",
    "\n",
    "\n",
    "    x_dim, y_dim, z_dim, t_dim = brain.shape\n",
    "\n",
    "    (brain_data.header).set_data_shape([x_dim,y_dim,z_dim,num_ROIs])\n",
    "\n",
    "    brain_roi_tensor = np.zeros((brain_data.header.get_data_shape()))\n",
    "    \n",
    "    print(\"Creating brain for Subject-\",sub_id)\n",
    "    for roi in range(num_ROIs):\n",
    "        brain_voxel_counter = 0\n",
    "        for i in range(x_dim):\n",
    "            for j in range(y_dim):\n",
    "                for k in range(z_dim):\n",
    "                    if mask[i,j,k] == 1:\n",
    "                        brain_roi_tensor[i,j,k,roi] = roi_brain_matrix[roi,brain_voxel_counter]\n",
    "                        brain_voxel_counter = brain_voxel_counter + 1\n",
    "\n",
    "        \n",
    "        assert (brain_voxel_counter == len(roi_brain_matrix[roi,:])) \n",
    "    print(\"Created brain for Subject-\",sub_id)\n",
    "\n",
    "\n",
    "    path = os.getcwd()\n",
    "    fc_file_name = fc_file_name + '.nii.gz'\n",
    "    out_file = opj(path,fc_file_name)\n",
    "    \n",
    "    brain_with_header = nib.Nifti1Image(brain_roi_tensor, affine=brain_data.affine,header = brain_data.header)\n",
    "    nib.save(brain_with_header,out_file)\n",
    "    \n",
    "    \n",
    "    fc_map_brain_file = out_file\n",
    "    return fc_map_brain_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again Create the Node and set default values to paths\n",
    "\n",
    "pearcoff = Node(Function(function=pear_coff, input_names=['in_file','atlas_file','mask_file'],\n",
    "                                output_names=['fc_map_brain_file']), name='pearcoff')\n",
    "\n",
    "\n",
    "# output_names=['fc_map_brain_file']\n",
    "# pearcoff.inputs.atlas_file = atlasPath\n",
    "# pearcoff.inputs.num_brain_voxels = num_brain_voxels\n",
    "# pearcoff.inputs.mask_file = mask_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT:\n",
    "* The ROI 255 has been removed due to resampling. Therefore the FC maps will have nan at that row. So don't use that ROI :)\n",
    "* I came to know coz I keep getting this error: RuntimeWarning: invalid value encountered in true_divide\n",
    "* To debug it, I read the coff matrix and checked its diagnol to discover the nan value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Node for applying xformation matrix to functional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func2std_xform = Node(FLIRT(output_type='NIFTI_GZ',\n",
    "                         apply_xfm=True), name=\"func2std_xform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pearcoff.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# motion_param_reg = [True, False]\n",
    "# global_signal_reg = [True, False]\n",
    "# band_pass_filt= [True, False]\n",
    "# for motion_param_regression, global_signal_regression, band_pass_filtering in zip(motion_param_reg, global_signal_reg, band_pass_filt):\n",
    "#     print (motion_param_regression, global_signal_regression, band_pass_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# itr = (list(itertools.product([0, 1], repeat=3)))\n",
    "\n",
    "# for motion_param_regression, global_signal_regression, band_pass_filtering in itr:\n",
    "#     print(motion_param_regression, global_signal_regression, band_pass_filtering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# itr = (list(itertools.product([0, 1], repeat=3)))\n",
    "\n",
    "# for motion_param_regression, global_signal_regression, band_pass_filtering in itr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the code to convert  the FC maps to brains instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination:  motionRegress1filt1global1\n",
      "171128-00:14:07,243 workflow INFO:\n",
      "\t Workflow temp_fc settings: ['check', 'execution', 'logging']\n",
      "171128-00:14:07,261 workflow INFO:\n",
      "\t Running in parallel.\n",
      "171128-00:14:07,265 workflow INFO:\n",
      "\t Executing: getSubjectFilenames.a1 ID: 0\n",
      "171128-00:14:07,268 workflow INFO:\n",
      "\t Executing: getSubjectFilenames.a0 ID: 6\n",
      "171128-00:14:07,297 workflow INFO:\n",
      "\t Executing node getSubjectFilenames.a1 in dir: /home1/varunk/results_again_again/temp_fc/_subject_id_0050003/getSubjectFilenames\n",
      "171128-00:14:07,305 workflow INFO:\n",
      "\t Executing node getSubjectFilenames.a0 in dir: /home1/varunk/results_again_again/temp_fc/_subject_id_0050002/getSubjectFilenames\n",
      "171128-00:14:07,633 workflow INFO:\n",
      "\t [Job finished] jobname: getSubjectFilenames.a1 jobid: 0\n",
      "171128-00:14:07,636 workflow INFO:\n",
      "\t Executing: calc_residuals.a1 ID: 1\n",
      "171128-00:14:07,653 workflow INFO:\n",
      "\t [Job finished] jobname: calc_residuals.a1 jobid: 1\n",
      "171128-00:14:07,680 workflow INFO:\n",
      "\t [Job finished] jobname: getSubjectFilenames.a0 jobid: 6\n",
      "171128-00:14:07,683 workflow INFO:\n",
      "\t Executing: globalSignalRemoval.a1 ID: 2\n",
      "171128-00:14:07,690 workflow INFO:\n",
      "\t [Job finished] jobname: globalSignalRemoval.a1 jobid: 2\n",
      "171128-00:14:07,691 workflow INFO:\n",
      "\t Executing: calc_residuals.a0 ID: 7\n",
      "171128-00:14:07,704 workflow INFO:\n",
      "\t [Job finished] jobname: calc_residuals.a0 jobid: 7\n",
      "171128-00:14:07,706 workflow INFO:\n",
      "\t Executing: bandpass.a1 ID: 3\n",
      "171128-00:14:07,714 workflow INFO:\n",
      "\t [Job finished] jobname: bandpass.a1 jobid: 3\n",
      "171128-00:14:07,715 workflow INFO:\n",
      "\t Executing: globalSignalRemoval.a0 ID: 8\n",
      "171128-00:14:07,722 workflow INFO:\n",
      "\t [Job finished] jobname: globalSignalRemoval.a0 jobid: 8\n",
      "171128-00:14:07,724 workflow INFO:\n",
      "\t Executing: pearcoff.a1 ID: 4\n",
      "171128-00:14:07,737 workflow INFO:\n",
      "\t Executing: bandpass.a0 ID: 9\n",
      "171128-00:14:07,739 workflow INFO:\n",
      "\t Executing node pearcoff.a1 in dir: /home1/varunk/results_again_again/temp_fc/_subject_id_0050003/pearcoff\n",
      "171128-00:14:07,745 workflow INFO:\n",
      "\t [Job finished] jobname: bandpass.a0 jobid: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:71: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ROIs are not present. Replacing NAN in coff matrix by 0\n",
      "Pear Matrix calculated for subject:  0050003\n",
      "Creating brain for Subject- 0050003\n",
      "171128-00:14:09,749 workflow INFO:\n",
      "\t Executing: pearcoff.a0 ID: 10\n",
      "171128-00:14:09,776 workflow INFO:\n",
      "\t Executing node pearcoff.a0 in dir: /home1/varunk/results_again_again/temp_fc/_subject_id_0050002/pearcoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:71: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ROIs are not present. Replacing NAN in coff matrix by 0\n",
      "Pear Matrix calculated for subject:  0050002\n",
      "Creating brain for Subject- 0050002\n",
      "Created brain for Subject- 0050003\n",
      "171128-00:14:55,214 workflow INFO:\n",
      "\t [Job finished] jobname: pearcoff.a1 jobid: 4\n",
      "171128-00:14:55,216 workflow INFO:\n",
      "\t Executing: func2std_xform.a1 ID: 5\n",
      "171128-00:14:55,232 workflow INFO:\n",
      "\t Executing node func2std_xform.a1 in dir: /home1/varunk/results_again_again/temp_fc/_subject_id_0050003/func2std_xform\n",
      "171128-00:14:55,238 workflow INFO:\n",
      "\t Running: flirt -in /home1/varunk/results_again_again/temp_fc/_subject_id_0050003/pearcoff/0050003_fc_map.nii.gz -ref /home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/coreg_reg/resample_mni/MNI152_T1_2mm_brain_resample.nii -out 0050003_fc_map_flirt.nii.gz -omat 0050003_fc_map_flirt.mat -applyxfm -init /home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/coreg_reg/_subject_id_0050003/concat_xform/sub-0050003_task-rest_run-1_bold_roi_st_mcf_mean_bet_flirt_sub-0050003_T1w_resample_brain_flirt.mat\n",
      "Created brain for Subject- 0050002\n",
      "171128-00:14:58,811 workflow INFO:\n",
      "\t [Job finished] jobname: pearcoff.a0 jobid: 10\n",
      "171128-00:14:58,815 workflow INFO:\n",
      "\t Executing: func2std_xform.a0 ID: 11\n",
      "171128-00:14:58,843 workflow INFO:\n",
      "\t Executing node func2std_xform.a0 in dir: /home1/varunk/results_again_again/temp_fc/_subject_id_0050002/func2std_xform\n",
      "171128-00:14:58,850 workflow INFO:\n",
      "\t Running: flirt -in /home1/varunk/results_again_again/temp_fc/_subject_id_0050002/pearcoff/0050002_fc_map.nii.gz -ref /home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/coreg_reg/resample_mni/MNI152_T1_2mm_brain_resample.nii -out 0050002_fc_map_flirt.nii.gz -omat 0050002_fc_map_flirt.mat -applyxfm -init /home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/coreg_reg/_subject_id_0050002/concat_xform/sub-0050002_task-rest_run-1_bold_roi_st_mcf_mean_bet_flirt_sub-0050002_T1w_resample_brain_flirt.mat\n",
      "171128-00:15:03,976 workflow INFO:\n",
      "\t [Job finished] jobname: func2std_xform.a1 jobid: 5\n",
      "171128-00:15:07,436 workflow INFO:\n",
      "\t [Job finished] jobname: func2std_xform.a0 jobid: 11\n",
      "171128-00:15:07,439 workflow INFO:\n",
      "\t Executing: save_file_list ID: 12\n",
      "171128-00:15:07,442 workflow INFO:\n",
      "\t Executing node save_file_list in dir: /home1/varunk/results_again_again/temp_fc/save_file_list\n",
      "######################## File List ######################: \n",
      " [ '/home1/varunk/results_again_again/temp_fc/_subject_id_0050002/func2std_xform/0050002_fc_map_flirt.nii.gz'\n",
      " '/home1/varunk/results_again_again/temp_fc/_subject_id_0050003/func2std_xform/0050003_fc_map_flirt.nii.gz']\n",
      "171128-00:15:07,453 workflow INFO:\n",
      "\t [Job finished] jobname: save_file_list jobid: 12\n",
      "171128-00:15:07,455 workflow INFO:\n",
      "\t Executing: datasink ID: 13\n",
      "171128-00:15:07,461 workflow INFO:\n",
      "\t Executing node datasink in dir: /home1/varunk/results_again_again/temp_fc/datasink\n",
      "171128-00:15:07,467 workflow INFO:\n",
      "\t [Job finished] jobname: datasink jobid: 13\n",
      "CPU times: user 304 ms, sys: 76 ms, total: 380 ms\n",
      "Wall time: 1min\n",
      "171128-00:15:07,760 workflow INFO:\n",
      "\t Generated workflow graph: /home1/varunk/results_again_again/temp_fc/graph.dot.png (graph2use=exec, simple_form=True).\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "motion_param_regression = 1\n",
    "band_pass_filtering = 1\n",
    "global_signal_regression = 1\n",
    "smoothing = 1\n",
    "\n",
    "combination = 'motionRegress' + str(int(motion_param_regression)) + 'filt' + \\\n",
    "              str(int(band_pass_filtering)) + 'global' + str(int(global_signal_regression)) + \\\n",
    "              'smoothing' + str(int(smoothing))\n",
    "        \n",
    "print(\"Combination: \",combination)\n",
    "\n",
    "wf = Workflow(name=functional_connectivity_directory)\n",
    "wf.base_dir = base_directory # Dir where all the outputs will be stored(inside BETFlow folder).\n",
    "\n",
    "wf.connect([(infosource , getSubjectFilenames, [('subject_id','subject_id')])])\n",
    "\n",
    "if motion_param_regression == 1 and global_signal_regression == 1 and band_pass_filtering == 1 and smoothing = 1:\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "        wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "        wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "        wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "        wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "        \n",
    "        wf.connect([( bandpass, spatialSmooth, [('out_file','in_file')])])\n",
    "        \n",
    "        wf.connect([( spatialSmooth, pearcoff, [('out_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        wf.connect([(pearcoff, func2std_xform, [('fc_map_brain_file','in_file')])])\n",
    "        wf.connect([(getSubjectFilenames, func2std_xform, [('func2std_mat','in_matrix_file')])])\n",
    "        wf.connect([(getSubjectFilenames, func2std_xform, [('MNI3mm_path','reference')])])\n",
    "        \n",
    "#         -- send out file to save file list and then save the outputs\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(func2std_xform,  save_file_list, [('out_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "\n",
    "        #  wf.connect([(bandpass,  dataSink, [('out_file','motionRegress_filt_global.@out_file')])])\n",
    "\n",
    "\n",
    "        # if motion_param_regression == 1 and global_signal_regression == 1:    \n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "        \n",
    "        from IPython.display import Image\n",
    "        wf.write_graph(graph2use='exec', format='png', simple_form=True)\n",
    "        file_name = opj(base_directory,functional_connectivity_directory,'graph_detailed.dot.png')\n",
    "        Image(filename=file_name)\n",
    "\n",
    "elif motion_param_regression == 1 and global_signal_regression == 1 and band_pass_filtering == 0 and smoothing = 1: # 110\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "        wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "        wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "#         wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "#         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "\n",
    "        wf.connect([( globalSignalRemoval, pearcoff, [('out_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "\n",
    "\n",
    "elif motion_param_regression == 1 and global_signal_regression == 0 and band_pass_filtering == 1  and smoothing = 1: # 101\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "#         wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "        wf.connect([(calc_residuals, bandpass, [('residual_file','in_file')])])\n",
    "        wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "\n",
    "        wf.connect([( bandpass, pearcoff, [('out_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})      \n",
    "\n",
    "elif motion_param_regression == 1 and global_signal_regression == 0 and band_pass_filtering == 0 and smoothing = 1: # 100\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "        wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "#         wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "#         wf.connect([(calc_residuals, bandpass, [('residual_file','in_file')])])\n",
    "#         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "\n",
    "        wf.connect([( calc_residuals, pearcoff, [('residual_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})   \n",
    "\n",
    "\n",
    "elif motion_param_regression == 0 and global_signal_regression == 1 and band_pass_filtering == 1 and smoothing = 1: # 011\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "        wf.connect([(getSubjectFilenames, globalSignalRemoval, [('brain','in_file')] )])\n",
    "        wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "        wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "        wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "\n",
    "        wf.connect([( bandpass, pearcoff, [('out_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})   \n",
    "\n",
    "\n",
    "\n",
    "elif motion_param_regression == 0 and global_signal_regression == 1 and band_pass_filtering == 0 and smoothing = 1: # 010\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "        wf.connect([(getSubjectFilenames, globalSignalRemoval, [('brain','in_file')] )])\n",
    "        wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "#         wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "#         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "\n",
    "        wf.connect([( globalSignalRemoval, pearcoff, [('out_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "\n",
    "\n",
    "elif motion_param_regression == 0 and global_signal_regression == 0 and band_pass_filtering == 1 and smoothing = 1: # 001\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('brain','in_file')] )])\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "\n",
    "        wf.connect([(getSubjectFilenames, bandpass, [('out_file','in_file')])])\n",
    "        wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "\n",
    "        wf.connect([( bandpass, pearcoff, [('out_file','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "\n",
    "else:\n",
    "\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('brain','in_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "        wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "\n",
    "        folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "\n",
    "        wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "\n",
    "        wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "        %time wf.run('MultiProc', plugin_args={'n_procs': 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '/home1/varunk/results_again_again/temp_fc/_subject_id_0050002/func2std_xform/0050002_fc_map_flirt.nii.gz',\n",
       "       '/home1/varunk/results_again_again/temp_fc/_subject_id_0050003/func2std_xform/0050003_fc_map_flirt.nii.gz'],\n",
       "      dtype='<U104')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(\"../results_again_again/temp_dataSink/pearcoff_motionRegress1filt1global1/fc_map_brain_file_list.npy\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': temp_fc.pearcoff.a1,\r\n",
      " 'traceback': ['Traceback (most recent call last):\\n',\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", '\r\n",
      "               'line 52, in run_node\\n'\r\n",
      "               \"    result['result'] = node.run(updatehash=updatehash)\\n\",\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", '\r\n",
      "               'line 372, in run\\n'\r\n",
      "               '    self._run_interface()\\n',\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", '\r\n",
      "               'line 482, in _run_interface\\n'\r\n",
      "               '    self._result = self._run_command(execute)\\n',\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", '\r\n",
      "               'line 613, in _run_command\\n'\r\n",
      "               '    result = self._interface.run()\\n',\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", '\r\n",
      "               'line 1081, in run\\n'\r\n",
      "               '    runtime = self._run_wrapper(runtime)\\n',\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", '\r\n",
      "               'line 1029, in _run_wrapper\\n'\r\n",
      "               '    runtime = self._run_interface(runtime)\\n',\r\n",
      "               '  File '\r\n",
      "               '\"/root/anaconda3/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", '\r\n",
      "               'line 194, in _run_interface\\n'\r\n",
      "               '    out = function_handle(**args)\\n',\r\n",
      "               '  File \"<string>\", line 155, in pear_coff\\n',\r\n",
      "               '  File \"<string>\", line 127, in convert_to_brain\\n',\r\n",
      "               \"NameError: name 'm' is not defined\\n\"\r\n",
      "               'Interface Function failed to run. \\n']}\r\n"
     ]
    }
   ],
   "source": [
    "!nipypecli show crash-20171125-133018-varunk-pearcoff.a1-7b869482-76ad-4f55-af87-8b01e34e975c.pklz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = np.load('../results_again_again/fc_datasink/pearcoff_motionRegress1filt1global1/fc_map_brain_file_list.npy')\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# elif motion_param_regression == True and global_signal_regression == True and band_pass_filtering == False: # 110\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "        \n",
    "#         wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "        \n",
    "# #         wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "# #         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "    \n",
    "#         wf.connect([( globalSignalRemoval, pearcoff, [('out_file','in_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "        \n",
    "        \n",
    "        \n",
    "#         wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "        \n",
    "#         wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "#         %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "        \n",
    "        \n",
    "# elif motion_param_regression == True and global_signal_regression == False and band_pass_filtering == True: # 101\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "        \n",
    "# #         wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "# #         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "        \n",
    "#         wf.connect([(calc_residuals, bandpass, [('residual_file','in_file')])])\n",
    "#         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "    \n",
    "#         wf.connect([( bandpass, pearcoff, [('out_file','in_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "        \n",
    "        \n",
    "        \n",
    "#         wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "        \n",
    "#         wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "#         %time wf.run('MultiProc', plugin_args={'n_procs': 7})      \n",
    "\n",
    "# elif motion_param_regression == True and global_signal_regression == False and band_pass_filtering == False: # 100\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "#         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "        \n",
    "# #         wf.connect([(calc_residuals, globalSignalRemoval, [('residual_file','in_file')] )])\n",
    "# #         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "        \n",
    "# #         wf.connect([(calc_residuals, bandpass, [('residual_file','in_file')])])\n",
    "# #         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "    \n",
    "#         wf.connect([( calc_residuals, pearcoff, [('residual_file','in_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "        \n",
    "        \n",
    "        \n",
    "#         wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "        \n",
    "#         wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "#         %time wf.run('MultiProc', plugin_args={'n_procs': 7})   \n",
    "        \n",
    "        \n",
    "# elif motion_param_regression == False and global_signal_regression == True and band_pass_filtering == True: # 011\n",
    "# #         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "# #         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "        \n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('brain','in_file')] )])\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "        \n",
    "#         wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "#         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "    \n",
    "#         wf.connect([( bandpass, pearcoff, [('out_file','in_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "        \n",
    "        \n",
    "        \n",
    "#         wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "        \n",
    "#         wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "#         %time wf.run('MultiProc', plugin_args={'n_procs': 7})   \n",
    "        \n",
    "        \n",
    "        \n",
    "# elif motion_param_regression == False and global_signal_regression == True and band_pass_filtering == False: # 010\n",
    "# #         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "# #         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "        \n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('brain','in_file')] )])\n",
    "#         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "        \n",
    "# #         wf.connect([(globalSignalRemoval, bandpass, [('out_file','in_file')])])\n",
    "# #         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "    \n",
    "#         wf.connect([( globalSignalRemoval, pearcoff, [('out_file','in_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "        \n",
    "        \n",
    "        \n",
    "#         wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "        \n",
    "#         wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "#         %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "        \n",
    "        \n",
    "# elif motion_param_regression == False and global_signal_regression == False and band_pass_filtering == True: # 001\n",
    "# #         wf.connect([(getSubjectFilenames, calc_residuals, [('brain','subject')])])\n",
    "# #         wf.connect([(getSubjectFilenames, calc_residuals, [('motion_param', 'motion_file')])])\n",
    "        \n",
    "# #         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('brain','in_file')] )])\n",
    "# #         wf.connect([(getSubjectFilenames, globalSignalRemoval, [('mask','mask_file')])])\n",
    "        \n",
    "# #         wf.connect([(getSubjectFilenames, bandpass, [('out_file','in_file')])])\n",
    "# #         wf.connect([(getSubjectFilenames, bandpass, [('tr','tr')])])\n",
    "    \n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('brain','in_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('atlas','atlas_file')])])\n",
    "#         wf.connect([( getSubjectFilenames, pearcoff, [('mask','mask_file')])])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folder_name = 'pearcoff_' + combination + '.@fc_map_brain_file'\n",
    "        \n",
    "        \n",
    "        \n",
    "#         wf.connect([(pearcoff,  save_file_list, [('fc_map_brain_file','in_fc_map_brain_file')])])\n",
    "\n",
    "        \n",
    "#         wf.connect([(save_file_list,  dataSink, [('out_fc_map_brain_file',folder_name)])])\n",
    "\n",
    "#         %time wf.run('MultiProc', plugin_args={'n_procs': 7})\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 111 -\n",
    "# 110 - \n",
    "# 101 - \n",
    "# 100\n",
    "# 011\n",
    "# 010\n",
    "# 001\n",
    "# 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the detailed graph\n",
    "# from IPython.display import Image\n",
    "# wf.write_graph(graph2use='exec', format='png', simple_form=True)\n",
    "# file_name = opj(base_directory,functional_connectivity_directory,'graph_detailed.dot.png')\n",
    "# Image(filename=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def calc_residuals(subject,\n",
    "#                    motion_file):\n",
    "#     \"\"\"\n",
    "#     Calculates residuals of nuisance regressors -motion parameters for every voxel for a subject using GLM.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     subject : string\n",
    "#         Path of a subject's motion corrected nifti file.\n",
    "#     motion_par_file : string\n",
    "#         path of a subject's motion parameters\n",
    "    \n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     residual_file : string\n",
    "#         Path of residual file in nifti format\n",
    "    \n",
    "#     \"\"\"\n",
    "#     import nibabel as nb\n",
    "#     nii = nb.load(subject)\n",
    "#     data = nii.get_data().astype(np.float32)\n",
    "#     global_mask = (data != 0).sum(-1) != 0\n",
    "\n",
    "    \n",
    "#     # Check and define regressors which are provided from files\n",
    "#     if motion_file is not None:\n",
    "#         motion = np.genfromtxt(motion_file)\n",
    "#         if motion.shape[0] != data.shape[3]:\n",
    "#             raise ValueError('Motion parameters {0} do not match data '\n",
    "#                              'timepoints {1}'.format(motion.shape[0], \n",
    "#                                                      data.shape[3]))\n",
    "#         if motion.size == 0:\n",
    "#             raise ValueError('Motion signal file {0} is '\n",
    "#                              'empty'.format(motion_file))\n",
    "\n",
    "#     # Calculate regressors\n",
    "#     regressor_map = {'constant' : np.ones((data.shape[3],1))}\n",
    "        \n",
    "#     regressor_map['motion'] = motion\n",
    "        \n",
    "    \n",
    "#     X = np.zeros((data.shape[3], 1))\n",
    "    \n",
    "#     for rname, rval in regressor_map.items():\n",
    "#         X = np.hstack((X, rval.reshape(rval.shape[0],-1)))\n",
    "\n",
    "#     X = X[:,1:]\n",
    "    \n",
    "#     if np.isnan(X).any() or np.isnan(X).any():\n",
    "#         raise ValueError('Regressor file contains NaN')\n",
    "\n",
    "#     Y = data[global_mask].T\n",
    "\n",
    "#     try:\n",
    "#         B = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "#     except np.linalg.LinAlgError as e:\n",
    "#         if \"Singular matrix\" in e:\n",
    "#             raise Exception(\"Error details: {0}\\n\\nSingular matrix error: \"\n",
    "#                             \"The nuisance regression configuration you \"\n",
    "#                             \"selected may have been too stringent, and the \"\n",
    "#                             \"regression could not be completed. Ensure your \"\n",
    "#                             \"parameters are not too \"\n",
    "#                             \"extreme.\\n\\n\".format(e))\n",
    "#         else:\n",
    "#             raise Exception(\"Error details: {0}\\n\\nSomething went wrong with \"\n",
    "#                             \"nuisance regression.\\n\\n\".format(e))\n",
    "\n",
    "#     Y_res = Y - X.dot(B)\n",
    "    \n",
    "#     data[global_mask] = Y_res.T\n",
    "    \n",
    "#     img = nb.Nifti1Image(data, header=nii.get_header(),\n",
    "#                          affine=nii.get_affine())\n",
    "    \n",
    "#     subject_name = subject.split('/')[-1].split('.')[0]\n",
    "#     filename = subject_name + '_residual.nii.gz'\n",
    "#     residual_file = os.path.join(os.getcwd(),filename )\n",
    "#     img.to_filename(residual_file) # alt to nib.save\n",
    "    \n",
    "#     return residual_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test\n",
    "# brain = '/home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/_subject_id_0050005/applyMask/sub-0050005_task-rest_run-1_bold_roi_st_mcf.nii_brain.nii.gz'\n",
    "# mask = '/home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/_subject_id_0050005/meanfuncmask/sub-0050005_task-rest_run-1_bold_roi_st_mcf_mean_brain_mask.nii.gz'\n",
    "# atlas = '/home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/coreg_reg/atlas_resize_reg_directory/_subject_id_0050005/std2func_xform/fullbrain_atlas_thr0-2mm_resample_flirt.nii '\n",
    "# # tr = 1.5\n",
    "# par_file = '/home1/varunk/results_again_again/ABIDE1_Preprocess/motion_correction_bet/_subject_id_0050005/mcflirt/sub-0050005_task-rest_run-1_bold_roi_st_mcf.nii.par'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calc_residuals(brain,par_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# motion = np.genfromtxt(par_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.load('../results_again_again/fc_datasink/pearcoff_motionRegress0filt0global1/fc_map_brain_file_list.npy')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
